{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjsKwxNPAkCT"
      },
      "source": [
        "# Chapter 10: PyTorch, calculus, and gradient descent\n",
        "\n",
        "Enough playing around. It's time to get serious. ðŸ˜°\n",
        "\n",
        "This programming assignment is our chance to implement the gradient-based optimization algorithms that we studied [in class](https://mml.johnmyersmath.com/stats-book/chapters/11-optim.html#). These implementations will use the differentiation engine built into the Python library called [PyTorch](https://pytorch.org/). This library is used in many cutting-edge, state-of-the-art machine learning models.\n",
        "\n",
        "PyTorch comes with its own new type of data structure, called _PyTorch tensors_, which you should think of as NumPy arrays with lots of extra functionality. Therefore, before we get busy with coding our gradient descent algorithms, we need to learn a bit about tensors. After that, you will learn how to use PyTorch to compute many of the objects that we discussed in class, like gradient vectors, Hessian matrices, eigenvalues and eigenvectors, condition numbers, _etc_.\n",
        "\n",
        "The second half of the assignment centers on gradient descent algorithms, both the basic non-stochastic version and the stochastic version. Starting from code templates that I provide, you will implement both of these algorithms and get a chance to experiment with them, helping build your intuition for parameter selection, diagnosing convergence, and other things.\n",
        "\n",
        "Have fun. ðŸ˜ˆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H4MC3jcArod"
      },
      "source": [
        "## Directions\n",
        "\n",
        "1. The programming assignment is organized into sequences of short problems. You can see the structure of the programming assignment by opening the \"Table of Contents\" along the left side of the notebook (if you are using Google Colab or Jupyter Lab).\n",
        "\n",
        "2. Do not add any cells of your own to the notebook, or delete any existing cells (either code or markdown)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaCDAKs3Atmx"
      },
      "source": [
        "## Submission instructions\n",
        "\n",
        "1. Once you have finished entering all your solutions, you will want to rerun all cells from scratch to ensure that everything works OK. To do this in Google Colab, click \"Runtime -> Restart and run all\" along the top of the notebook.\n",
        "\n",
        "2. Now scroll back through your notebook and make sure that all code cells ran properly.\n",
        "\n",
        "3. If everything looks OK, save your assignment and upload the `.ipynb` file at the provided link on the course <a href=\"https://github.com/jmyers7/stats-book-materials\">GitHub repo</a>. Late submissions are not accepted.\n",
        "\n",
        "4. You may submit multiple times, but I will only grade your last submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJuECPr2tVtc"
      },
      "source": [
        "## PyTorch basics\n",
        "\n",
        "The first half of this programming assignment centers on the basics of PyTorch tensors, and using them to compute various objects of interest in calculus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6HqHzzwA2ds"
      },
      "source": [
        "### Tensors\n",
        "\n",
        "As I mentioned in the introduction, you should think of a PyTorch tensor like a NumPy array, but with lots of extra functionality. In particular, PyTorch tensors are specifically built to be fed into the PyTorch differentiation engine, as we will see later.\n",
        "\n",
        "To get started, let's first import PyTorch by calling `import torch`. Then, let's define a $2$-dimensional tensor $\\mathbf{A}$ (i.e., a matrix) by calling the `torch.tensor` constructor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "922RNnVrBZga",
        "outputId": "2a8706dd-7eea-4857-c816-ba7dcc9ff0a9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "A = torch.tensor([[1, 2], [3, 4]])\n",
        "A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5fWPApPCMOS"
      },
      "source": [
        "Let's check its shape (i.e., size) and dimension by accessing the `shape` attribute and calling the `dim` method (just like in NumPy):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjpaApJUCQXp",
        "outputId": "808490c6-d135-4afe-969b-adb0958da086"
      },
      "outputs": [],
      "source": [
        "print(A.shape)\n",
        "print(A.dim())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iofN_bfJBqKS"
      },
      "source": [
        "Most of the basic operations that you can do with NumPy arrays, you can also do with PyTorch tensors. For example, let's multiply two $2$-dimensional tensors (i.e., matrices) using the `@` operator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpRReSdpByPB",
        "outputId": "58083c9a-cf5c-4819-c804-ec2a999f51cc"
      },
      "outputs": [],
      "source": [
        "B = torch.tensor([[0, 1, -1], [3, 4, 5]])\n",
        "A @ B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iuGZhxU829T"
      },
      "source": [
        "Vectors are $1$-dimensional tensors, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-raw9B788Vj",
        "outputId": "2240da38-08b3-4fab-a7b0-4f636bd903ec"
      },
      "outputs": [],
      "source": [
        "v = torch.tensor([-1, 3])\n",
        "v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCpPSlqF9Z6l"
      },
      "source": [
        "PyTorch does not express a preference for either column or row vectors, and instead uses $1$-dimensional tensors in their place. This is what `v` is. Let's check its shape and dimension:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9VsKO-59cdH",
        "outputId": "0e8c2b42-c4e2-428f-debd-80744ac1bf50"
      },
      "outputs": [],
      "source": [
        "print(v.shape)\n",
        "print(v.dim())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npKpGdFb9jcy"
      },
      "source": [
        "Note that the shape of `v` is just $2$ (as a $1$-dimensional tensor), not $2\\times 1$ or $1\\times 2$, as you would expect for either a column or a row vector.  You can _force_ `v` to be a column vector by reshaping it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3VAYBtg_iV0",
        "outputId": "6fc54e37-8989-4f62-92fe-4a9ef67d5130"
      },
      "outputs": [],
      "source": [
        "v.reshape(2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGLn9z_rB-IK"
      },
      "source": [
        "Or, you can force `v` to be row vector by reshaping in the other direction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps-osEXW_kKf",
        "outputId": "3e4ff52c-8c4e-4bef-9727-4e8c095f9365"
      },
      "outputs": [],
      "source": [
        "v.reshape(1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7mlkjXM_416"
      },
      "source": [
        "One place where using $1$-dimensional tensors in place of column and row vectors can be confusing is in the lack of transpose in certain operations involving vectors and matrices. For example, the Euclidean inner product between two (column) vectors in $\\mathbb{R}^n$ is given by the matrix product $\\mathbf{v}^\\intercal \\mathbf{w}$. This assumes that $\\mathbf{v}$ and $\\mathbf{w}$ are both column vectors, of size $n\\times 1$. But if these vectors are implemented as $1$-dimensional tensors in PyTorch, the tranpose is not needed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfAQ1k_pC6Ey",
        "outputId": "f4999e0c-ebc1-4cfc-a837-18cb1eec6a2a"
      },
      "outputs": [],
      "source": [
        "v = torch.tensor([1, 2, 4])\n",
        "w = torch.tensor([0, -1, 2])\n",
        "v @ w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvoafBFPDLid"
      },
      "source": [
        "Likewise, if $\\mathbf{v}$ is $n\\times 1$ and $\\mathbf{A}$ is $n\\times m$, then we may form the product $\\mathbf{v}^\\intercal \\mathbf{A}$ to obtain a $1\\times m$ row vector. But if $\\mathbf{v}$ is a $1$-dimensional tensor, you don't need the transpose:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyoHcy2VDKw2",
        "outputId": "fc550655-3caf-4393-cf9d-ab1803e49cec"
      },
      "outputs": [],
      "source": [
        "A = torch.tensor([[1, 2], [0, -1], [3, 1]])   # 3x2 matrix\n",
        "v @ A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLFekh8YDdfW"
      },
      "source": [
        "#### Problem 1 --- Basic tensor algebra\n",
        "\n",
        "Consider the matrices given by\n",
        "\n",
        "$$\n",
        "\\mathbf{A} = \\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\ 0 & -1 & 4\n",
        "\\end{bmatrix}, \\quad\n",
        "\\mathbf{B} = \\begin{bmatrix}\n",
        "1 & 3 \\\\\n",
        "-1 & 10 \\\\\n",
        "0 & 5\n",
        "\\end{bmatrix}, \\quad\n",
        "\\mathbf{C} = \\begin{bmatrix}\n",
        "0 & -2 \\\\\n",
        "3 & -4 \\\\\n",
        "0 & 7\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "In the next code cell, implement all three as $2$-dimensional tensors, calling them `A`, `B`, and `C`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHE-mOcFD0UU"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAC8Y2plEVuS"
      },
      "source": [
        "Now, in the next code cell, compute the product $\\mathbf{A} \\mathbf{B}$ and the linear combination $2\\mathbf{B}+3\\mathbf{C}$. Print your answers so that I can see them both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MSuLH73D5_Q",
        "outputId": "74c950bc-bd16-47ce-d6ff-e7ab22171942"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv5VPdyMEb94"
      },
      "source": [
        "Suppose that we have the vector $\\mathbf{v}^\\intercal = \\begin{bmatrix} 2 & 3 \\end{bmatrix}$. In the next code cell, compute the product $\\mathbf{v}^\\intercal \\mathbf{A}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WaKqhyhEnpy",
        "outputId": "997fe6b3-c73b-4c74-e2a0-8ce23df1f644"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th1hvzU9HBPU"
      },
      "source": [
        "### Partial derivatives and gradients\n",
        "\n",
        "Consider the function\n",
        "\n",
        "$$\n",
        "f:\\mathbb{R}^3 \\to \\mathbb{R}, \\quad f(x,y,z) = 6(x^2+y) z^3.\n",
        "$$\n",
        "\n",
        "As you may easily compute by hand, we have\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial f}{\\partial x}(x, y, z) &= 12xz^3 &\\Rightarrow \\frac{\\partial f}{\\partial x}(1, -2, 2) &= 96, \\\\\n",
        "\\frac{\\partial f}{\\partial y}(x, y, z) &= 6z^3 &\\Rightarrow \\frac{\\partial f}{\\partial y}(1, -2, 2) &= 48, \\\\\n",
        "\\frac{\\partial f}{\\partial z}(x, y, z) &= 18(x^2+y)z^2 &\\Rightarrow \\frac{\\partial f}{\\partial z}(1, -2, 2) &= -72.\n",
        "\\end{align*}\n",
        "\n",
        "These partial derivatives may be computed in PyTorch as follows. Run the cell, and then read the description of each step below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rr1uFlWH-7D",
        "outputId": "02b32da9-90c4-441d-853d-1b3fcb7b064f"
      },
      "outputs": [],
      "source": [
        "# step 1\n",
        "def f(x, y, z):\n",
        "  return 6 * (x ** 2 + y) * z ** 3\n",
        "\n",
        "# step 2\n",
        "x = torch.tensor([1.], requires_grad=True)\n",
        "y = torch.tensor([-2.], requires_grad=True)\n",
        "z = torch.tensor([2.], requires_grad=True)\n",
        "\n",
        "# step 3\n",
        "objective = f(x, y, z)\n",
        "\n",
        "# step 4\n",
        "objective.backward()\n",
        "\n",
        "# step 5\n",
        "print(x.grad)\n",
        "print(y.grad)\n",
        "print(z.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6EDBpLuIWph"
      },
      "source": [
        "Here's the explanation for each step:\n",
        "\n",
        "1. First, define the function $f$ as a Python function.\n",
        "2. Define three tensors representing the input $(x,y,z) = (1, -2, 2)$ to the partial derivatives. The tensors must have a floating point data type, so  integers must be followed by a trailing decimal point. We must also set `requires_grad=True` in order to access the partial derivatives later.\n",
        "3. Compute the functional value $f(1, -2, 2)$, storing it in the variable `objective`.\n",
        "4. Compute the partial derivatives by calling the `backward` method on `objective`.\n",
        "5. Access the partial derivatives via the `grad` attributes on the `x`, `y`, and `z` tensors.\n",
        "\n",
        "The reason that the method (in step 4) for computing derivatives is called `backward` comes from terminology in deep learning (in particular, the so-called \"backward\" pass during the [backpropagation algorithm](https://en.wikipedia.org/wiki/Backpropagation)).\n",
        "\n",
        "Alternatively, we may assemble the individual inputs $x$, $y$, and $z$ together into an input vector:\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\theta}^\\intercal = \\begin{bmatrix} x & y & z \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Then, we may redefine $f$ as a function of $\\boldsymbol{\\theta}$ and compute the gradient vector $\\nabla f(1, -2, 2)$ in one shot. Run the next code cell to see how this is done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNP2RKCEKoFL",
        "outputId": "6490967b-f4d8-4090-c3b5-ae31ddedaa62"
      },
      "outputs": [],
      "source": [
        "def f(theta):\n",
        "  x = theta[0]\n",
        "  y = theta[1]\n",
        "  z = theta[2]\n",
        "  return 6 * (x ** 2 + y) * z ** 3\n",
        "\n",
        "theta = torch.tensor([1., -2., 2.], requires_grad=True)\n",
        "\n",
        "objective = f(theta)\n",
        "\n",
        "objective.backward()\n",
        "\n",
        "theta.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XpOS5cmLk9y"
      },
      "source": [
        "#### Problem 2 --- Computing gradients\n",
        "\n",
        "In the next code cell, I have implemented an objective function of the form $J: \\mathbb{R}^2 \\to \\mathbb{R}$ for you, as it is somewhat complicated. I wanted to give you a function with interesting contours! So, all you need to do is execute this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJQmgsFlLqaP"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "\n",
        "def J(theta):\n",
        "   theta1 = (sqrt(2) / 2) * (theta[0] - theta[1])\n",
        "   theta2 = (sqrt(2) / 2) * (theta[0] + theta[1])\n",
        "   return (theta1 ** 2 + 10 * theta2 ** 2) * ((theta1 - 1) ** 2 + 10 * (theta2 - 1) ** 2) * ((theta1 + 1) ** 2 + 10 * (theta2 - 2) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zap7TXLRNsRO"
      },
      "source": [
        "To get an idea of the shape of the graph of $J$, let's look at its contours:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "MpISQDHrMaUV",
        "outputId": "0a55fa16-85f0-471d-e6bf-e339ca88b90d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x, y = np.mgrid[-1.2:2.8:0.01, -1.2:2.8:0.01]\n",
        "grid = np.dstack((x, y))\n",
        "z = np.apply_along_axis(J, axis=-1, arr=grid)\n",
        "\n",
        "contour = plt.contour(x, y, z, levels=np.arange(0, 1000, 100), cmap='plasma')\n",
        "plt.clabel(contour, fontsize=8)\n",
        "plt.xlabel('$\\\\theta_1$')\n",
        "plt.ylabel('$\\\\theta_2$')\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXo3l2-pQVn1"
      },
      "source": [
        "In the next code cell, you will compute the gradient vector $\\nabla J(1, 1)$ by using the template I provided above. Store the functional value $J(1, 1)$ in the variable `objective`, and store your gradient in the variable `grad`. Be sure to print `grad`, so that I can see it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l69SvmBOt3N",
        "outputId": "cdb13dbb-2f32-4ab3-a255-2f3058a18a02"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTaBYueMQymE"
      },
      "source": [
        "Now, normalize the gradient so that it has magnitude $1$, and multiply it by a negative to reverse its direction. Save your answer in the variable `unit_neg_grad`, and be sure to print it out. (You'll have to look up on your own how to normalize a $1$-dimensional tensor in PyTorch.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHcnN14QQsGp",
        "outputId": "a5fa8cad-aaaf-4944-ddc2-5bf9c20148ee"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQvwa8TzRYmV"
      },
      "source": [
        "Let's check your answer. When you execute the next cell, you should see that your unit negative gradient vector is orthogonal to the contour passing through its tail and that it is pointing downhill."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "IjOpLgpMRhNK",
        "outputId": "9c313a06-25f4-4d5d-9e3d-6474ac069d34"
      },
      "outputs": [],
      "source": [
        "theta = torch.tensor([1., 1.], requires_grad=True)\n",
        "objective = J(theta)\n",
        "objective.backward()\n",
        "unit_neg_grad = -theta.grad / torch.linalg.norm(theta.grad)\n",
        "\n",
        "plt.contour(x, y, z, levels=np.arange(0, 1000, 100), cmap='plasma')\n",
        "plt.contour(x, y, z, levels=[objective.detach().numpy()])\n",
        "plt.quiver(*theta.detach().numpy(), *unit_neg_grad, angles='xy', scale_units='xy', scale=1, zorder=3)\n",
        "plt.xlabel('$\\\\theta_1$')\n",
        "plt.ylabel('$\\\\theta_2$')\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2PDVPqmSCND"
      },
      "source": [
        "In the next code cell, compute the minimum rate of change of $J$ at the point $\\boldsymbol{\\theta} = (1, 1)$. (You'll first need to remember from class how we find this rate.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nemv43u_Senj",
        "outputId": "da2e8638-34ca-4dc6-a421-1166974b51c4"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnRQpdVATwMW"
      },
      "source": [
        "### Hessian matrices\n",
        "\n",
        "Let's return to our function\n",
        "\n",
        "$$\n",
        "f:\\mathbb{R}^3 \\to \\mathbb{R}, \\quad f(x,y,z) = 6(x^2+y) z^3,\n",
        "$$\n",
        "\n",
        "discussed at the beginning of the previous section. We compute its Hessian matrix by hand as:\n",
        "\n",
        "$$\n",
        "\\nabla^2 f(x, y, z) = \\begin{bmatrix}\n",
        "12 z^3 & 0 & 36xz^2 \\\\\n",
        "0 & 0 & 18z^2 \\\\\n",
        "36xz^2 & 18z^2 & 36(x^2+y)z\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Thus,\n",
        "\n",
        "$$\n",
        "\\nabla^2 f(1, -2, 2) = \\begin{bmatrix}\n",
        "96 & 0 & 144 \\\\ 0 & 0 & 72 \\\\ 144 & 72 & -72\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "The next code cell shows how these computations are implemented in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTBg_bW8UNGC",
        "outputId": "7081f736-925a-4722-e6a4-6387630cbcca"
      },
      "outputs": [],
      "source": [
        "from torch.autograd.functional import hessian\n",
        "\n",
        "theta = torch.tensor([1., -2., 2.])\n",
        "hess = hessian(f, theta)\n",
        "hess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krCLpLsNVotE"
      },
      "source": [
        "Let's get the eigenvalues and eigenvectors of the Hessian matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLftfucCVlcZ",
        "outputId": "c3a3d723-8f8d-4785-9dca-9a71eedacbff"
      },
      "outputs": [],
      "source": [
        "# hide warnings about casting complex numbers to floats. you only need to import this once.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "evals, evecs = torch.linalg.eig(hess)\n",
        "evals = evals.float()\n",
        "evecs = evecs.float()\n",
        "print(evals)\n",
        "print(evecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "784Ds8aYWKqR"
      },
      "source": [
        "Notice that `evals` is a $1$-dimensional tensor containing the eigenvalues, while `evecs` is a $2$-dimensional tensor whose columns are the corresponding (unit) eigenvectors.\n",
        "\n",
        "Recall from class that the eigenvalues are the curvatures of the graph of $f$ at the point $(1, -2,2)$ in the directions specified by the corresponding eigenvectors. Let's double check that this is true, by using the formula\n",
        "\n",
        "$$\n",
        "f''_\\mathbf{v}(1,-2,2) = \\mathbf{v}^\\intercal \\left(\\nabla^2 f(1, -2, 2)\\right) \\mathbf{v}\n",
        "$$\n",
        "\n",
        "that we learned in class. Supposing that $\\mathbf{v}$ is the second eigenvector, here's the code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTvog8afWf2Q",
        "outputId": "157e60e8-8fb5-4bef-d17d-ed4034cdfdf2"
      },
      "outputs": [],
      "source": [
        "v = evecs[:, 1]\n",
        "v @ hess @ v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBvTmAghXmfP"
      },
      "source": [
        "Notice that, indeed, the curvature is given by the corresponding eigenvalue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KfVd7DIYI-b"
      },
      "source": [
        "#### Problem 3 --- Computing Hessian matrices\n",
        "\n",
        "We return to the function $J$ from Problem 2. In the next code cell, compute the Hessian matrix $\\nabla^2 J(\\sqrt{2},0)$. Save your answer into the variable `hess`, and be sure to print it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6nnEVmRYphD",
        "outputId": "6eb0e6c8-23af-4ec4-a64d-a1638af3ff6c"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd3VIOYaayne"
      },
      "source": [
        "Now, compute the eigenvalues and eigenvectors of the Hessian matrix. Be sure to convert them to floats as I did in the example above, and save them into the variables `evals` and `evecs`. Print them out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKYc6MGmZGn5",
        "outputId": "b1146554-680b-4656-d3d7-8433158a096f"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1oFIZGJbCi0"
      },
      "source": [
        "Let's check your answer. Assuming you coded everything correctly, when you execute the next code cell you should see your eigenvectors with their tails at the point $(\\sqrt{2},0)$ on the contour plot of the function $J$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "r_21gM0fZRvc",
        "outputId": "45030a3a-e31d-48a9-d687-87d42d00fcb4"
      },
      "outputs": [],
      "source": [
        "x_evecs = evecs[0, :]\n",
        "y_evecs = evecs[1, :]\n",
        "\n",
        "x, y = np.mgrid[-1.2:2.8:0.01, -1.2:2.8:0.01]\n",
        "grid = np.dstack((x, y))\n",
        "z = np.apply_along_axis(J, axis=-1, arr=grid)\n",
        "\n",
        "plt.contour(x, y, z, levels=np.arange(0, 1000, 100), cmap='plasma')\n",
        "plt.contour(x, y, z, levels=[objective.detach().numpy()])\n",
        "plt.quiver([sqrt(2)] * 2, [0] * 2, x_evecs, y_evecs, angles='xy', scale_units='xy', scale=1, zorder=3)\n",
        "plt.xlabel('$\\\\theta_1$')\n",
        "plt.ylabel('$\\\\theta_2$')\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO5BAD9bbRm1"
      },
      "source": [
        "Notice that the eigenvectors are orthogonal to each other (since the Hessian matrix is symmetric). The eigenvector pointing in the northwesterly direction has an eigenvalue of $\\approx 308$, while the eigenvector pointing in the southwesterly direction has an eigenvalue of $\\approx 3080$. These are curvatures!\n",
        "\n",
        "Even though you already know the condition number of the Hessian matrix $\\nabla^2 J(\\sqrt{2},0)$ because you already computed its spectrum, in the next code cell I want you to hunt for a way to compute the condition number directly using a method from PyTorch. Once you find the appropriate method, compute the condition number, saving it into the variable `condition_num`, and then print it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQJG5WKDcW0d",
        "outputId": "f1e715cc-f7c2-4cbd-d0bb-9a45ca49dd82"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnOteIdfeLRa"
      },
      "source": [
        "## Optimization algorithms\n",
        "\n",
        "We will now work towards coding implementations of gradient descent and stochastic gradient descent, from scratch. While the algorithms are relatively straightforward, coding them can be a challenge for new coders. This is mainly because we want the algorithms to output more than just a single point estimate for the minimizer, but also the entire sequence of approximations produced by the algorithm along with all the associated objective values, and handling all the logistics of tracking these things is a bit of a headache. But don't worry!---I will help you along by providing templates for the algorithms, and your only job is to fill in the missing fragments of code.\n",
        "\n",
        "The focus in this assignment is making sure we understand the internal workings of the basic gradient descent algorithms by coding them by hand. In the real world, unless you're a researcher inventing your own optimization algorithms, you would use algorithms that have been pre-coded for you, like those that come bundled with PyTorch. (Click [here](https://pytorch.org/docs/stable/optim.html#algorithms) to see the list of optimization algorithms in PyTorch.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ1YnaXbXuKt"
      },
      "source": [
        "### Gradient descent\n",
        "\n",
        "We begin with the basic multi-variable gradient descent algorithm described in the textbook [here](https://mml.johnmyersmath.com/stats-book/chapters/11-optim.html#gd-alg). Make sure to walk through the textbook description multiple times, in detail, before attempting this portion of the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k3iA8wcX1Kn"
      },
      "source": [
        "#### Problem 4 --- Implementing gradient descent\n",
        "\n",
        "In the next code cell, I have coded a template for the gradient descent algorithm. The template is in the form of a function, called `GD`, that we will call whenever we need the algorithm. Your job is to complete the template to get a functioning implementation. But before doing that, let's discuss the call signature of the `GD` function:\n",
        "\n",
        "`GD(J, theta0, lr, num_steps, decay_rate=0)`\n",
        "\n",
        "The parameters are:\n",
        "\n",
        "* `J`, for the objective function $J$\n",
        "* `theta0` for the intial guess $\\boldsymbol{\\theta}_0$\n",
        "* `lr` for the learning rate $\\alpha$\n",
        "* `num_steps` for the number $N$ of gradient steps\n",
        "* `decay_rate` for the decay rate $\\beta$, set to a default value of `0`\n",
        "\n",
        "I recommend that you read through _all_ of the code in the template to get a sense of what's going on, but you only need to fill the four `None` placeholders in the code labeled block 1 through block 4. Here are the descriptions of these four blocks:\n",
        "\n",
        "1. Begin the main `for` loop.\n",
        "\n",
        "2. Compute the gradient of the objective with resepect to $\\boldsymbol{\\theta}$.\n",
        "\n",
        "3. Take a gradient step according to the update rule.\n",
        "\n",
        "4. Compute an updated objective value using the new $\\boldsymbol{\\theta}$.\n",
        "\n",
        "You need to fill in the missing code using these descriptions! Be very careful to **only** replace the `None`'s that are marked block 1 through block 4. You will see other `None`'s in the class `GD_output`, but do **not** replace these!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Oh_jvcbT0ko"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# do NOT replace the `None`'s in this class!!!\n",
        "@dataclass\n",
        "class GD_output:\n",
        "    thetas: torch.Tensor = None\n",
        "    objectives: torch.Tensor = None\n",
        "    per_step_objectives: torch.Tensor = None\n",
        "    per_epoch_objectives: torch.Tensor = None\n",
        "    epoch_step_nums: torch.Tensor = None\n",
        "    lr: float = None\n",
        "    num_steps: int = None\n",
        "    decay_rate: float = None\n",
        "    batch_size: int = None\n",
        "    num_epochs: int = None\n",
        "    max_steps: int = None\n",
        "\n",
        "    def __str__(self):\n",
        "      if self.per_step_objectives == None:\n",
        "        return f'learning rate: {self.lr}, gradient steps: {self.num_steps}, decay rate: {self.decay_rate}' \\\n",
        "        f'\\n\\nthetas:\\n{self.thetas}\\n\\nobjectives:\\n{self.objectives}'\n",
        "      else:\n",
        "        return f'learning rate: {self.lr}, number of epochs: {self.num_epochs}, batch size: {self.batch_size}, decay rate: {self.decay_rate}' \\\n",
        "        f'\\n\\nthetas:\\n{self.thetas}\\n\\nper-step objectives:\\n{self.per_step_objectives}' \\\n",
        "        f'\\n\\nper-epoch mean objectives:\\n{self.per_epoch_objectives}' \\\n",
        "        f'\\n\\nepochs completed on the follow gradient steps:\\n{self.epoch_step_nums}'\n",
        "\n",
        "\n",
        "# begin the gradient descent function\n",
        "def GD(J, theta0, lr, num_steps, decay_rate=0):\n",
        "\n",
        "    theta = theta0.clone().requires_grad_()\n",
        "    objective = J(theta)\n",
        "    objectives = [objective.detach()]\n",
        "    thetas = [theta.detach().clone()]\n",
        "\n",
        "    # block 1\n",
        "    None        # <-- replace `None` with your own code\n",
        "\n",
        "        # block 2\n",
        "        None    # <-- replace `None` with your own code\n",
        "\n",
        "        # block 3\n",
        "        with torch.no_grad():\n",
        "            theta -= None         # <-- replace `None` with your own code\n",
        "\n",
        "        # block 4\n",
        "        None                      # <-- replace `None` with your own code\n",
        "\n",
        "        objectives.append(objective.detach())\n",
        "        thetas.append(theta.detach().clone())\n",
        "        theta.grad.zero_()\n",
        "\n",
        "    thetas = torch.row_stack(thetas)\n",
        "    objectives = torch.stack(objectives)\n",
        "    output = GD_output(thetas=thetas,\n",
        "                       objectives=objectives,\n",
        "                       lr=lr,\n",
        "                       num_steps=num_steps,\n",
        "                       decay_rate=decay_rate)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW7-WBeIeL3K"
      },
      "source": [
        "To check your work, we will return to the objective function $J$ from Problems 2 and 3 above, and use the gradient descent algorithm to find the local minimum at $(\\sqrt{2},0)$. Execute the next cell, which runs the algorithm with learning rate $\\alpha = 0.001$ over $N=25$ gradient steps beginning from the initial value $\\boldsymbol{\\theta}_0=(1,1)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f32KSvkwjPVr"
      },
      "outputs": [],
      "source": [
        "# gradient descent parameters\n",
        "theta0 = torch.tensor([1., 1.])\n",
        "alpha = 0.001\n",
        "N = 25\n",
        "\n",
        "# run the algorithm, collect the output into `gd_output`\n",
        "gd_output = GD(J=J, theta0=theta0, lr=alpha, num_steps=N)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrCsT-Z6jRQG"
      },
      "source": [
        "Our implementation of the `GD` function returns an object of the custom class `GD_output`, which holds the approximations\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1,\\ldots,\\boldsymbol{\\theta}_N\n",
        "$$\n",
        "\n",
        "along with the objective values\n",
        "\n",
        "$$\n",
        "J(\\boldsymbol{\\theta}_0),J(\\boldsymbol{\\theta}_1),\\ldots,J(\\boldsymbol{\\theta}_N).\n",
        "$$\n",
        "\n",
        "Let's check the contents of the output of the algorithm by printing the `gd_output` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y00JmQ9RjVx7",
        "outputId": "dcc35fc7-867a-427c-f829-7f47acd442f8"
      },
      "outputs": [],
      "source": [
        "print(gd_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPYrElcnjdMq"
      },
      "source": [
        "If your code is correct, then you should see sensible output. The rows of the first tensor in the printout are the $\\boldsymbol{\\theta}$'s generated by the algorithm, while the second tensor contains the associated objective values.\n",
        "\n",
        "But plots would be much more informative! Since we will need to plot the outputs from multiple runs of the algorithm, let's code a utility function for plotting. Execute the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2XoCtAed4AT"
      },
      "outputs": [],
      "source": [
        "def plot_gd_output(gd_output):\n",
        "  fig, axes = plt.subplots(ncols=2, figsize=(9, 4))\n",
        "\n",
        "  # plot the trace of the algorithm\n",
        "  axes[0].contour(x, y, z, levels=np.arange(0, 1000, 100), cmap='plasma')\n",
        "  axes[0].plot(gd_output.thetas[:, 0], gd_output.thetas[:, 1])\n",
        "  axes[0].scatter(gd_output.thetas[:, 0], gd_output.thetas[:, 1], s=30, zorder=3)\n",
        "  axes[0].set_xlabel('$\\\\theta_1$')\n",
        "  axes[0].set_ylabel('$\\\\theta_2$')\n",
        "  axes[0].set_aspect('equal')\n",
        "\n",
        "  # plot objective versus gradient steps\n",
        "  axes[1].plot(range(N + 1), gd_output.objectives)\n",
        "  axes[1].scatter(range(N + 1), gd_output.objectives)\n",
        "  axes[1].set_xlabel('gradient steps')\n",
        "  axes[1].set_ylabel('objective')\n",
        "\n",
        "  fig.suptitle(f'$\\\\alpha={gd_output.lr}$, $\\\\beta={gd_output.decay_rate}$, $N={gd_output.num_steps}$')\n",
        "  plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNiyjuD9nYBk"
      },
      "source": [
        "Now, using our helper function, let's check your code for the gradient descent algorithm. Execute the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "8hoyUIQwhrkR",
        "outputId": "747f1bf2-d410-4c92-8785-ab2d8fb0796e"
      },
      "outputs": [],
      "source": [
        "plot_gd_output(gd_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-MyC_LcnrV0"
      },
      "source": [
        "You should see that the algorithm finds the general neighborhood of the minimizer at $(\\sqrt{2},0)$, but then oscillates back and forth (mostly in the direction of maximum curvature) and never fully converges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHA25XJKn8yO"
      },
      "source": [
        "#### Problem 5 --- Obtaining convergence\n",
        "\n",
        "Let's alter some of the parameters for the gradient descent algorithm in order to obtain convergence. In the next code cell, choose a suitable value for the decay rate $\\beta$ so that the algorithm converges to the local minimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "Aszz3TFOoDQF",
        "outputId": "da29f7ea-6572-43c0-afef-2b08aedf7331"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "# gradient descent parameters\n",
        "theta0 = torch.tensor([1., 1.])\n",
        "alpha = 0.001\n",
        "N = 25\n",
        "beta = None         # <-- replace `None` with your own code\n",
        "\n",
        "gd_output = None    # <-- replace `None` with your own code\n",
        "\n",
        "# plot the output\n",
        "None                # <-- replace `None` with your own code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xgX3YzUpw5G"
      },
      "source": [
        "Rather than increasing the decay rate in order to encourange convergence and dampen oscillations, we saw in class (in [this](https://mml.johnmyersmath.com/stats-book/chapters/11-optim.html#quadratic-conv-thm) theorem) that we can set the learning rate to the reciprocal mean curvature of the graph of $J$ at the local minimum. In the next code cell, set the learning rate $\\alpha$ to this value and run the algorithm. (Use $\\beta=0$.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "k46CtcEKoyT0",
        "outputId": "1532a2e1-1b21-4abb-b764-92c1d57f0165"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "# gradient descent parameters\n",
        "theta0 = torch.tensor([1., 1.])\n",
        "alpha = None        # <-- replace `None` with your own code\n",
        "N = 25\n",
        "\n",
        "gd_output = None    # <-- replace `None` with your own code\n",
        "\n",
        "# plot the output\n",
        "None                # <-- replace `None` with your own code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cxcixazqXD2"
      },
      "source": [
        "Lovely convergence!\n",
        "\n",
        "In the next code cell, we print the last $\\boldsymbol{\\theta}$ produced by the algorithm as our final approximation to the minimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt5SJ1aMsZRR",
        "outputId": "ebe5dc81-5746-47d2-832f-e101f48e42e8"
      },
      "outputs": [],
      "source": [
        "print('approximate minimizer found by gradient descent:', gd_output.thetas[-1].tolist())\n",
        "print('true minimizer:                                 ', [np.sqrt(2), 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8ReUYRBTRB6"
      },
      "source": [
        "### Stochastic gradient descent\n",
        "\n",
        "We now turn toward the stochastic gradient descent algorithm, as described in the textbook [here](https://mml.johnmyersmath.com/stats-book/chapters/11-optim.html#sgd-alg). Of course, the biggest difference between this version of gradient descent and the non-stochastic one is that the data must be (randomly) partitioned into mini-batches over each epoch. To do this, we will use a built-in utility from PyTorch from the DataLoader class (see [here](https://pytorch.org/docs/stable/data.html#)). Other than that, the basic pieces of the stochastic version of the algorithm are more or less the same as the non-stochastic one.\n",
        "\n",
        "But as we talked about in class, to monitor convergence of the stochastic version of the algorithm, it is convenient to track _both_ the per-step objective values and also the mean per-epoch objective values. This adds a level of complexity to the implementation that is not present in the non-stochastic version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lE54WCRTVXv"
      },
      "source": [
        "#### Problem 6 --- Implementing stochastic gradient descent\n",
        "\n",
        "The format of this problem is exactly the same as Problem 4. But this time, we will code a function, called `SGD`, which implements _stochastic_ gradient descent. Its call signature will be:\n",
        "\n",
        "`SGD(g, theta0, X, lr, batch_size, num_epochs, decay_rate=0, max_steps=-1, shuffle=True, random_state=None)`\n",
        "\n",
        "The parameters are:\n",
        "\n",
        "* `g` for the funtion described in the textbook [here](https://mml.johnmyersmath.com/stats-book/chapters/11-optim.html#sgd-alg)\n",
        "\n",
        "* `theta0` for the initial $\\boldsymbol{\\theta}_0$\n",
        "\n",
        "* `X` for a $2$-dimensional tensor (i.e., matrix) whose rows are the dataset $\\mathbf{x}_1,\\mathbf{x}_2\\ldots,\\mathbf{x}_m$\n",
        "\n",
        "* `lr` for the learning rate $\\alpha$\n",
        "\n",
        "* `batch_size` for the mini-batch size $k$\n",
        "\n",
        "* `num_epochs` for the number of epochs $N$\n",
        "\n",
        "* `decay_rate` for the decay rate $\\beta$\n",
        "\n",
        "* `max_steps` for a parameter that halts the algorithm at a specified number of gradient steps\n",
        "\n",
        "There are two more parameters, `shuffle` and `random_state`, that you don't need to worry about.\n",
        "\n",
        "As the stochastic version of the algorithm is more complex than the basic version, so too will our code be more complex. The template below contains the basic skeleton of the code. As in Problem 4, your job is only to replace five `None` placeholders in the code labeled block 1 through block 4. Here are the descriptions of these blocks:\n",
        "\n",
        "1. Begin the outer and inner `for` loops through the epochs and mini-batches.\n",
        "\n",
        "2. Compute the gradient of the objective with respect to $\\boldsymbol{\\theta}$.\n",
        "\n",
        "3. Take a gradient step using the update rule.\n",
        "\n",
        "4. Compute the new objective value with the updated $\\boldsymbol{\\theta}$.\n",
        "\n",
        "Using these descriptions, fill in your code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQTC2Mf6TZQb"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def SGD(g, theta0, X, lr, batch_size, num_epochs, decay_rate=0, max_steps=-1, shuffle=True, random_state=42):\n",
        "\n",
        "    if random_state != None:\n",
        "        torch.manual_seed(random_state)\n",
        "    data_loader = DataLoader(dataset=X, batch_size=batch_size, shuffle=shuffle)\n",
        "    num_mini_batches = len(data_loader)\n",
        "\n",
        "    theta = theta0.clone().requires_grad_()\n",
        "    per_step_objectives = []\n",
        "    per_epoch_objectives = []\n",
        "    thetas = [theta.detach().clone()]\n",
        "\n",
        "    first_step = True\n",
        "    s = 0\n",
        "    epoch_step_nums = [0]\n",
        "\n",
        "    # block 1\n",
        "    None        # <-- replace `None` with your own code\n",
        "\n",
        "        for i, mini_batch in enumerate(data_loader):\n",
        "\n",
        "            if first_step:\n",
        "                objective = g(mini_batch, theta).mean()\n",
        "                per_step_objectives.append(objective.detach())\n",
        "                per_epoch_objectives.append(objective.detach())\n",
        "                first_step = False\n",
        "\n",
        "            # block 2\n",
        "            None        # <-- replace `None` with your own code\n",
        "\n",
        "            # block 3\n",
        "            None        # <-- replace `None` with your own code\n",
        "            None        # <-- replace `None` with your own code\n",
        "\n",
        "            # block 4\n",
        "            None        # <-- replace `None` with your own code\n",
        "\n",
        "            per_step_objectives.append(objective.detach())\n",
        "            thetas.append(theta.detach().clone())\n",
        "            theta.grad.zero_()\n",
        "\n",
        "            complete_epoch = True if i + 1 == num_mini_batches else False\n",
        "            s += 1\n",
        "            if s == max_steps:\n",
        "                break\n",
        "\n",
        "        if complete_epoch:\n",
        "            epoch_step_nums.append(s)\n",
        "\n",
        "        epoch_objective = torch.tensor(per_step_objectives[num_mini_batches * t + 1:])\n",
        "        per_epoch_objectives.append(epoch_objective.mean())\n",
        "\n",
        "        if s == max_steps:\n",
        "            break\n",
        "\n",
        "    thetas = torch.row_stack(thetas)\n",
        "    per_step_objectives = torch.stack(per_step_objectives)\n",
        "    per_epoch_objectives = torch.stack(per_epoch_objectives)\n",
        "    epoch_step_nums = torch.tensor(epoch_step_nums)\n",
        "    output = GD_output(thetas=thetas,\n",
        "                       per_step_objectives=per_step_objectives,\n",
        "                       per_epoch_objectives=per_epoch_objectives,\n",
        "                       epoch_step_nums=epoch_step_nums,\n",
        "                       lr=lr,\n",
        "                       num_epochs=num_epochs,\n",
        "                       decay_rate=decay_rate,\n",
        "                       batch_size=batch_size,\n",
        "                       max_steps=max_steps)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFOnLE5OqzIx"
      },
      "source": [
        "In order to check whether your code is correct, we need a dataset and stochastic objective function. To get the dataset, run the next code cell, which generates the same dataset drawn from a 2-dimensional normal distribution used in the [textbook](https://mml.johnmyersmath.com/stats-book/chapters/11-optim.html#stochastic-gradient-descent):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "RfLMKJ1Xmm3u",
        "outputId": "47e5c97b-6f80-46e3-f661-6a32c8411c2c"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "torch.manual_seed(42)\n",
        "X = MultivariateNormal(loc=torch.zeros(2), covariance_matrix=torch.eye(2)).sample(sample_shape=(1024,))\n",
        "\n",
        "sns.scatterplot(x=X[:, 0], y=X[:, 1])\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.gcf().set_size_inches(w=5, h=3)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3uofpRm5YAS"
      },
      "source": [
        "Notice that the dataset is loaded into a $2$-dimensional tensor `X` (i.e., a matrix).\n",
        "\n",
        "Now, in the next code cell, I want you to implement the function\n",
        "\n",
        "$$\n",
        "g:\\mathbb{R}^4 \\to \\mathbb{R}, \\quad g(\\mathbf{x};\\boldsymbol{\\theta}) = \\frac{1}{2}\\left[ (x_1 - \\theta_1)^2 + 4(x_2 - \\theta_2)^2\\right]\n",
        "$$\n",
        "\n",
        "along with the associated stochastic objective function\n",
        "\n",
        "$$\n",
        "J:\\mathbb{R}^2 \\to \\mathbb{R}, \\quad J(\\boldsymbol{\\theta}) = \\frac{1}{m} \\sum_{i=1}^m g(\\mathbf{x}_i;\\boldsymbol{\\theta}).\n",
        "$$\n",
        "\n",
        "Once you enter your code and run the cell, you should see a plot of the contours of $J$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "k9wxeCOl9IEM",
        "outputId": "d496e8d6-dca4-49b5-a7ef-466ca5f78777"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "def g(X, theta):\n",
        "    x1 = X[:, 0]\n",
        "    x2 = X[:, 1]\n",
        "    theta1 = theta[0]\n",
        "    theta2 = theta[1]\n",
        "    return None         # <-- replace `None` with your own code\n",
        "\n",
        "def J(theta):\n",
        "    return None         # <-- replace `None` with your own code\n",
        "\n",
        "x, y = np.mgrid[-2:2:0.05, -2:2:0.05]\n",
        "grid = np.dstack([x, y])\n",
        "z = np.apply_along_axis(J, axis=-1, arr=grid)\n",
        "\n",
        "contour = plt.contour(x, y, z, levels=np.arange(0, 10, 0.5), cmap='plasma')\n",
        "plt.clabel(contour)\n",
        "plt.xlabel('$\\\\theta_1$')\n",
        "plt.ylabel('$\\\\theta_2$')\n",
        "plt.gca().set_aspect('equal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xAZzYuUq9pr"
      },
      "source": [
        "Now that we have a dataset, a function $g$, and the stochastic objective function $J$, we may run our stochastic gradient descent algorithm. Do so in the following cell, by filling in the `None` placeholder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlQxN-Plmsel"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "# stochastic gradient descent parameters\n",
        "theta0 = torch.tensor([1.5, 1.5])\n",
        "alpha = 0.3\n",
        "k = 128\n",
        "N = 4\n",
        "\n",
        "# run the algorithm, collect the output into `gd_output`\n",
        "gd_output = None        # <-- replace `None` with your own code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3-Zzztb6XQN"
      },
      "source": [
        "Let's check the output. Run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWVzk5v58nVp",
        "outputId": "e875c6f4-12d6-46ad-b8d8-f47ad5990ebf"
      },
      "outputs": [],
      "source": [
        "print(gd_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXI2Umad6jGD"
      },
      "source": [
        "This printout is nice, but a visualization would be better. Run the next cell, which implements a helper function for plotting the output of the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZROmfX_IrZIk"
      },
      "outputs": [],
      "source": [
        "def plot_sgd_output(gd_output):\n",
        "  fig, axes = plt.subplots(ncols=2, figsize=(9, 4))\n",
        "\n",
        "  # plot the trace of the algorithm\n",
        "  axes[0].contour(x, y, z, levels=np.arange(0, 10, 0.25), cmap='plasma')\n",
        "  axes[0].plot(gd_output.thetas[:, 0], gd_output.thetas[:, 1])\n",
        "  axes[0].scatter(gd_output.thetas[:, 0], gd_output.thetas[:, 1], s=30, zorder=3)\n",
        "  axes[0].set_xlabel('$\\\\theta_1$')\n",
        "  axes[0].set_ylabel('$\\\\theta_2$')\n",
        "  axes[0].set_aspect('equal')\n",
        "\n",
        "  # plot objective versus gradient steps\n",
        "  axes[1].plot(range(len(gd_output.per_step_objectives)), gd_output.per_step_objectives, alpha=0.25, label='objective per step')\n",
        "  axes[1].scatter(range(len(gd_output.per_step_objectives)), gd_output.per_step_objectives, alpha=0.25)\n",
        "  axes[1].plot(gd_output.epoch_step_nums, gd_output.per_epoch_objectives, label='mean objective per epoch')\n",
        "  axes[1].scatter(gd_output.epoch_step_nums, gd_output.per_epoch_objectives, zorder=3)\n",
        "  axes[1].set_xlabel('gradient steps')\n",
        "  axes[1].set_ylabel('objective')\n",
        "  axes[1].legend()\n",
        "\n",
        "  fig.suptitle(f'$\\\\alpha={gd_output.lr}$, $\\\\beta={gd_output.decay_rate}$, $k={gd_output.batch_size}$, $N={gd_output.num_epochs}$')\n",
        "  plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_weE-uY6qyd"
      },
      "source": [
        "Using the helper function, in the next code cell, pass in the output of the algorithm to obtain a visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "Wdo7mvOciEH9",
        "outputId": "cb93b39d-752f-41f9-8799-bae9ada5e0ff"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlhccHhU8BMK"
      },
      "source": [
        "#### Problem 7 --- Experimenting with convergence\n",
        "\n",
        "In the next code cell, try selecting different values for the size $k$ of the mini-batches and plotting the results. What happens if you select $k=1$, so that the algorithm takes a gradient step per data point? What happens if you choose $k=1{,}024$, so that the algorithm turns into [batch gradient descent](https://mml.johnmyersmath.com/stats-book/chapters/11-optim.html#batch-gd-def)? What happens for values in between these two extremes? For your final answer, select your favorite mini-batch size out of all the ones you try. ðŸ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "ql7WnOUT7EA1",
        "outputId": "d67125f8-7c96-4573-ea7b-c937e5d6237e"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "# stochastic gradient descent parameters\n",
        "theta0 = torch.tensor([1.5, 1.5])\n",
        "alpha = 0.3\n",
        "k = None            # <-- replace `None` with your own code\n",
        "N = 8\n",
        "\n",
        "# run the algorithm, collect the output into `gd_output`\n",
        "gd_output = None    # <-- replace `None` with your own code\n",
        "\n",
        "# plot the results\n",
        "None                # <-- replace `None` with your own code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
