{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aP7X9BJS-7C"
      },
      "source": [
        "# Chapter 13: Training probabilistic graphical models\n",
        "\n",
        "In the [programming assignment](https://github.com/jmyers7/stats-book-materials/blob/main/programming-assignments/assignment_12.ipynb) for the [previous chapter](https://mml.johnmyersmath.com/stats-book/chapters/12-models.html), we trained probabilistic models using the [scikit-learn](https://scikit-learn.org/stable/index.html) library. Part of what makes that library so powerful and useful is that it hides the training process from the user, allowing the user to focus on extracting insights and enlightenment from their data without worrying about technical stuff. But, at least once in their career, analysts should see what it takes to train a probabilistic model _from scratch_. That's what we're going to do in this assignment.\n",
        "\n",
        "In the previous programming assignment, we trained a Naive Bayes model to function as a spam classifier. Our goal is to re-do the construction of that model, but this time we will train the model ourselves by minimizing the cross entropy training objective (see [Theorem 13.4](https://mml.johnmyersmath.com/stats-book/chapters/13-learning.html#equiv-obj-gen-thm) in the book) via stochastic gradient descent (SGD). Our workflow is this:\n",
        "\n",
        "1. Implement the link function for the Naive Bayes model.\n",
        "2. Implement the [model surprisal function](https://mml.johnmyersmath.com/stats-book/chapters/13-learning.html#gen-model-functions-def) to serve as the [loss function](https://mml.johnmyersmath.com/stats-book/chapters/11-optim.html#stochastic-gradient-descent) for SGD.\n",
        "3. Minimize the [cross entropy](https://mml.johnmyersmath.com/stats-book/chapters/13-learning.html#equiv-obj-gen-thm) via SGD.\n",
        "4. Check for convergence of SGD using diagnostic plots.\n",
        "5. Evaluate the goodness-of-fit of the model by computing classification metrics like the _confusion matrix_, _accuracy_, _precision_, and _recall_.\n",
        "\n",
        "This general workflow is not special to Naive Bayes models---this is more or less the same sequence of steps that you would follow to train _any_ model.\n",
        "\n",
        "Let's get started!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN-PywswTOwD"
      },
      "source": [
        "## Directions\n",
        "\n",
        "1. The programming assignment is organized into sequences of short problems. You can see the structure of the programming assignment by opening the \"Table of Contents\" along the left side of the notebook (if you are using Google Colab or Jupyter Lab).\n",
        "\n",
        "2. Do not add any cells of your own to the notebook, or delete any existing cells (either code or markdown)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwXqbgBsTRh-"
      },
      "source": [
        "## Submission instructions\n",
        "\n",
        "1. Once you have finished entering all your solutions, you will want to rerun all cells from scratch to ensure that everything works OK. To do this in Google Colab, click \"Runtime -> Restart and run all\" along the top of the notebook.\n",
        "\n",
        "2. Now scroll back through your notebook and make sure that all code cells ran properly.\n",
        "\n",
        "3. If everything looks OK, save your assignment and upload the `.ipynb` file at the provided link on the course <a href=\"https://github.com/jmyers7/stats-book-materials\">GitHub repo</a>. Late submissions are not accepted.\n",
        "\n",
        "4. You may submit multiple times, but I will only grade your last submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNgoCS_9TYBT"
      },
      "source": [
        "## The cross entropy training objective for a Naive Bayes model\n",
        "\n",
        "We first saw _Naive Bayes models_ back in the [programming assignment](https://github.com/jmyers7/stats-book-materials/blob/main/programming-assignments/assignment_12.ipynb) for Chapter 12, and we studied their likelihood and surprisal functions in the [worksheet](https://github.com/jmyers7/stats-book-materials/blob/main/worksheets/13-learning-sol.pdf) for the current chapter. Recall that the underlying graph of a Naive Bayes model is of the form\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/jmyers7/stats-book-materials/main/img/nb.svg\" width=\"200\" align=\"center\">\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "where $\\mathbf{X} \\in \\mathbb{R}^n$. The parameters are given by a number $\\psi\\in [0,1]$ that parametrizes the distribution of $Y\\sim \\mathcal{B}er(\\psi)$, as well as two vectors $\\boldsymbol{\\theta}_0, \\boldsymbol{\\theta}_1 \\in [0,1]^n$. The link function at $\\mathbf{X}$ is given by\n",
        "\n",
        "$$\n",
        "p(\\mathbf{x} \\mid y ; \\  \\boldsymbol{\\theta}_0, \\boldsymbol{\\theta}_1 ) = \\prod_{j=1}^n \\phi_j^{x_j}(1-\\phi_j)^{1-x_j} \\quad \\text{where} \\quad \\boldsymbol{\\phi} = (1-y) \\boldsymbol{\\theta}_0 + y \\boldsymbol{\\theta}_1\n",
        "$$\n",
        "\n",
        "and\t$\\boldsymbol{\\phi}^\\intercal = (\\phi_1,\\ldots,\\phi_n)$.\n",
        "\n",
        "Given an observed dataset\n",
        "\n",
        "$$\n",
        "(\\mathbf{x}_1,y_1),(\\mathbf{x}_2,y_2),\\ldots,(\\mathbf{x}_m,y_m) \\in \\{0,1\\}^n \\times \\{0,1\\},\n",
        "$$\n",
        "\n",
        "we saw in Problem 1 of the [worksheet](https://github.com/jmyers7/stats-book-materials/blob/main/worksheets/13-learning-sol.pdf) (for the current chapter) that the cross entropy stochastic objective function is given by\n",
        "\n",
        "$$\n",
        "J(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1) = E_{(\\mathbf{x},y)\\sim \\hat{p}(\\mathbf{x},y)} \\left[\\mathcal{I}_\\text{model}(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1; \\ \\mathbf{x},y) \\right] = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{I}_\\text{model}(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1; \\ \\mathbf{x}_i,y_i),\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\mathcal{I}_\\text{model}(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1; \\ \\mathbf{x}_i,y_i) = - y_i \\log{\\psi} - (1-y_i) \\log{(1-\\psi)} - \\sum_{j=1}^n\\left[x_{ij} \\log{\\phi_j} + (1-x_{ij}) \\log{(1-\\phi_j)} \\right]\n",
        "$$\n",
        "\n",
        "is the _model surprisal function_ evaluated on the $i$-th instance of the dataset (for $i=1,\\ldots,m$) and where\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_i^\\intercal = (x_{i1},x_{i2},\\ldots,x_{in})\n",
        "$$\n",
        "\n",
        "is the _feature vector_ for the $i$-th instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5oo26vUOAFw"
      },
      "source": [
        "## The idea of \"vectorization\"\n",
        "\n",
        "So, it would seem that all we need to do is code the model surprisal function, and then toss it into the stochastic gradient descent algorithm as the [loss function](https://mml.johnmyersmath.com/stats-book/chapters/11-optim.html#stochastic-gradient-descent) to learn the parameters from the data. But, for the most efficient implementation, we need our model surprisal function to be \"vectorized.\"\n",
        "\n",
        "To explain what this means, it will be convenent to recall that the _design matrix_ of the dataset is the $m\\times n$ matrix $\\mathcal{X}$ with the feature vectors $\\mathbf{x}_i$ as rows:\n",
        "\n",
        "$$\n",
        "\\mathcal{X} \\stackrel{\\text{def}}{=}\n",
        "\\begin{bmatrix}\n",
        "\\leftarrow & \\mathbf{x}_1^\\intercal & \\rightarrow \\\\\n",
        "\\leftarrow & \\mathbf{x}_2^\\intercal & \\rightarrow \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "\\leftarrow & \\mathbf{x}_m^\\intercal & \\rightarrow\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} & \\cdots & x_{1n} \\\\\n",
        "x_{21} & x_{22} & \\cdots & x_{2n} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "x_{m1} & x_{m2} & \\cdots & x_{mn}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "We also imagine taking all the $y_i$'s and loading them into a single $m\\times 1$ column vector\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "that we call the _(true) label vector_. Now, to say that the model surprisal function should be _vectorized_ means that we can plug the _entire_ design matrix $\\mathcal{X}$ and the _entire_ label vector $\\mathbf{y}$ into it as arguments, producing the following vector as output:\n",
        "\n",
        "$$\n",
        "\\mathcal{I}_\\text{model}(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1; \\ \\mathcal{X},\\mathbf{y}) =\n",
        "\\begin{bmatrix}\n",
        "\\mathcal{I}_\\text{model}(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1; \\ \\mathbf{x}_1, y_1) \\\\\n",
        "\\mathcal{I}_\\text{model}(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1; \\ \\mathbf{x}_2, y_2) \\\\\n",
        "\\vdots \\\\\n",
        "\\mathcal{I}_\\text{model}(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1; \\ \\mathbf{x}_m, y_m)\n",
        "\\end{bmatrix} \\in \\mathbb{R}^m.\n",
        "$$\n",
        "\n",
        "Notice that the output is a column vector containing the surprisals of each of the $m$ instances of data.\n",
        "\n",
        "So, the bulk of the work in this assignment is designing a fully _vectorized_ version of the model surprisal function. I will walk you through it, step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szwoLd0VZvPN"
      },
      "source": [
        "## Importing the data\n",
        "\n",
        "First, let's import the email data from the previous [programming assignment](https://github.com/jmyers7/stats-book-materials/blob/main/programming-assignments/assignment_12.ipynb), as well as import (most of) the libraries required for this assignment. Run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "AsWXToptP9ix",
        "outputId": "f0fe944b-e056-4040-e413-31324dcfa4eb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "!pip install math_stats_ml>=0.0.18  # install the custom library for our course\n",
        "from math_stats_ml.gd import SGD, plot_gd # import the functions for training\n",
        "from math_stats_ml.autograders.assignment_13 import * # import autograders\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/jmyers7/stats-book-materials/main/data/data-12-3.csv'\n",
        "df = pd.read_csv(url)\n",
        "print('\\nThe email data:\\n')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inwxEEdnQ6JR"
      },
      "source": [
        "Remember that the dataset consists of observations of seven random variables $X_1,X_2,X_3,X_4,X_5,X_6$, and $Y$. The $X$'s are binary random variables that indicate ($1=$ yes and $0=$ no) whether the words\n",
        "\n",
        "$$\n",
        "\\text{office, cash, vacation, meeting, credit, cat}\n",
        "$$\n",
        "\n",
        "occur in an email, while $Y$ indicates whether the email is spam ($Y=1$) or not spam ($Y=0$).\n",
        "\n",
        "Our design matrix $\\mathcal{X}$ consists of the $X$-columns in the dataframe, while our label vector $\\mathbf{y}$ consists of the $Y$-column. Let's extract these columns from the dataframe, and turn them into PyTorch tensors. I will do this for you. Run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nIipBXKQG4l",
        "outputId": "3e0d440b-d726-4952-8693-69ab9982cecf"
      },
      "outputs": [],
      "source": [
        "X = torch.tensor(df.iloc[:, :6].to_numpy(), dtype=torch.float32)\n",
        "y = torch.tensor(df['y'].to_numpy(), dtype=torch.float32)\n",
        "\n",
        "# print the design matrix `X` and vector `y`\n",
        "print('The design matrix X: \\n', X)\n",
        "print('\\nThe class label vector y: \\n', y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRZZXmnmTBZu"
      },
      "source": [
        "Notice the design matrix $\\mathcal{X}$ is assigned to the Python variable `X`, while the label vector $\\mathbf{y}$ is assigned to `y`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvQ1QZaxZxvo"
      },
      "source": [
        "## Building the vectorized link function\n",
        "\n",
        "Our first step in building a vectorized model surprisal function is building a vectorized link function. From our discussion above, the link function is given by\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\phi} = (1-y) \\boldsymbol{\\theta}_0 + y \\boldsymbol{\\theta}_1.\n",
        "$$\n",
        "\n",
        "In terms of components, this equation yields\n",
        "\n",
        "$$\n",
        "\\phi_j = (1-y) \\theta_{0j} + y \\theta_{1j} \\tag{1}\n",
        "$$\n",
        "\n",
        "for each $j=1,\\ldots,n$, where\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\theta}_0^\\intercal = (\\theta_{01},\\theta_{02},\\ldots,\\theta_{0n}) \\quad \\text{and} \\quad \\boldsymbol{\\theta}_1^\\intercal = (\\theta_{11},\\theta_{12},\\ldots,\\theta_{1n}).\n",
        "$$\n",
        "\n",
        "But these formulas are correct _only_ for a single instance $y\\in \\{0,1\\}$. To get the correct vectorized formulas, we need to bring in all the $y$'s from the dataset:\n",
        "\n",
        "$$\n",
        "y_1,y_2,\\ldots,y_m \\in \\{0,1\\}.\n",
        "$$\n",
        "\n",
        "If we write $y_i$ for the $i$-th instance in the dataset, then our formula (1) above needs to be rewritten as\n",
        "\n",
        "$$\n",
        "\\phi_{ij} = (1-y_i) \\theta_{0j} + y_i \\theta_{1j}.\n",
        "$$\n",
        "\n",
        "Notice now that the $\\phi$'s are doubly-indexed, with the first index $i$ (with $i=1,\\ldots,m$) picking out the $i$-th instance in the dataset, and the second index $j$ (with $j=1,\\ldots,n$) picking out the components of the parameter vectors $\\boldsymbol{\\theta}_0$ and $\\boldsymbol{\\theta}_1$. Thus, the $\\phi_{ij}$'s naturally combine to form a matrix of size $m\\times n$.\n",
        "\n",
        "But how do we get this matrix?\n",
        "\n",
        "To help you along, I'm going to show you the formula, but it'll be up to you to figure out the implementation. Here is the matrix:\n",
        "\n",
        "$$\n",
        "\\text{matrix of $\\phi_{ij}$'s} = (\\boldsymbol{1} - \\mathbf{y})\\boldsymbol{\\theta}_0^\\intercal + \\mathbf{y} \\boldsymbol{\\theta}_1^\\intercal. \\tag{2}\n",
        "$$\n",
        "\n",
        "In this formula, the bold $\\boldsymbol{1}$ represents an $m\\times 1$ column vector of all $1$'s. Take a moment or two to convince yourself that this expression really does yield the correct $\\phi_{ij}$'s.\n",
        "\n",
        "So, all you need to do is use formula (2) for your vectorized link function. To test out your implementation after you write it, let's get some random values for the parameter vectors $\\boldsymbol{\\theta}_0$ and $\\boldsymbol{\\theta}_1$. I will do this for you, as well as place the vectors in a [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) called `parameters`. Run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaSkBLAJUmdd"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(57702)\n",
        "theta0 = torch.rand(size=(6,))\n",
        "theta1 = torch.rand(size=(6,))\n",
        "parameters = {'theta0': theta0, 'theta1': theta1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udiQTWIav_lJ"
      },
      "source": [
        "You can access the individual parameters in the dictionary `parameters` by indexing into it using the name of the parameter, exactly like indexing into the columns of a dataframe. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6RuOs2-wK9h",
        "outputId": "2c9d500d-8d87-4d9e-9ac6-bd8e23b8aa9f"
      },
      "outputs": [],
      "source": [
        "parameters['theta0']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daVYVCK2XJ-E"
      },
      "source": [
        "Our code creates a parameter tensor `theta0` that consists of six random numbers drawn from the uniform distribution on the interval $[0,1)$. Same for `theta1`. Crucially, notice that the shapes of the parameter tensors are `(6,)` and `(6,)`.\n",
        "\n",
        "Also, the shape of the vector `y` is computed in the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng-koGZTXYmz",
        "outputId": "87b6faf8-f2e0-4aba-aa99-c1532a23e1e4"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geubYiC4Xaq5"
      },
      "source": [
        "Thus, the shape of `y` is `(512,)`. These observations are **very** important for getting a working implementation! Remember them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3ECp5lOWrg3"
      },
      "source": [
        "### Problem 1 --- Implementing the vectorized link function\n",
        "\n",
        "In the next cell, write your implementation of the vectorized link function.\n",
        "\n",
        "_Here are some hints:_\n",
        "\n",
        "1. Look at the code in [the book](https://mml.johnmyersmath.com/stats-book/chapters/13-learning.html#mle-for-logistic-regression) to see how I implemented the link function for a logistic regression model. Use this code as inspiration.\n",
        "2. Referring to formula (2), you may simply write `1 - y` to stand for $\\boldsymbol{1} - \\mathbf{y}$.\n",
        "3. Use the symbol `@` for matrix multiplication.\n",
        "4. To get the multiplications to work correctly, you may need to reshape your tensors from shapes `(6,)` and `(512,`) to shapes `(1, 6)` and `(512, 1)` by using the `reshape` method from PyTorch. Here's how it works: If `T` is a tensor of shape `(n,)` and you want to turn it into a column vector of shape `(n, 1`), write `T.reshape(-1, 1)`. If you want to turn it into a row vector of shape `(1, n)`, write `T.reshape(1, -1)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK9Lip5zZ0Sb"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "def phi_link(parameters, y):\n",
        "  None          # <-- replace `None` with your own code\n",
        "  None          # <-- replace `None` with your own code\n",
        "  return None   # <-- replace `None` with your own code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJUq-9Gf2gjb"
      },
      "source": [
        "Now, let's test to make sure your implementation is correct. In the next code cell, pass in the `parameters` dictionary we defined above and the vector `y` to your `phi_link` function. Save the output of `phi_link` into the variable `phi`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qypBCLMUzQrs"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "phi = None      # <-- replace `None` with your own code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3SwnL7e2qwA"
      },
      "source": [
        "Now, run the next code cell to check the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7TuXEKa0iZO",
        "outputId": "c06ddf08-2e0b-4ace-f30a-195a06d18e9b"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL TO CHECK YOUR ANSWERS\n",
        "\n",
        "prob_check(answers=[phi], prob_num=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1XU3rcG6T2M"
      },
      "source": [
        "## Building the vectorized model surprisal function\n",
        "\n",
        "We must now take your vectorized link function and build the full vectorized model surprisal function. For your reference, here is the formula from above, evaluated on a single instance $(\\mathbf{x}_i,y_i)$ of data:\n",
        "\n",
        "$$\n",
        "\\mathcal{I}_\\text{model}(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1; \\ \\mathbf{x}_i,y_i) = - y_i \\log{\\psi} - (1-y_i) \\log{(1-\\psi)} - \\sum_{j=1}^n\\left[x_{ij} \\log{\\phi_j} + (1-x_{ij}) \\log{(1-\\phi_j)} \\right].\n",
        "$$\n",
        "\n",
        "Remember, our goal is to write an implementation that takes the entire $m\\times n$ design matrix $\\mathcal{X}$ as input, along with the $m\\times 1$ label vector $\\mathbf{y}$:\n",
        "\n",
        "$$\n",
        "\\mathcal{I}_\\text{model}(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1; \\ \\mathcal{X},\\mathbf{y}) =\n",
        "\\begin{bmatrix}\n",
        "\\mathcal{I}_\\text{model}(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1; \\ \\mathbf{x}_1, y_1) \\\\\n",
        "\\mathcal{I}_\\text{model}(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1; \\ \\mathbf{x}_2, y_2) \\\\\n",
        "\\vdots \\\\\n",
        "\\mathcal{I}_\\text{model}(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1; \\ \\mathbf{x}_m, y_m)\n",
        "\\end{bmatrix} \\in \\mathbb{R}^m.\n",
        "$$\n",
        "\n",
        "But, using vector/matrix algebra, this may be rewritten as\n",
        "\n",
        "$$\n",
        "\\mathcal{I}_\\text{model}(\\psi,\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1; \\ \\mathcal{X},\\mathbf{y}) = -\\mathbf{y} \\log{\\psi} - (\\boldsymbol{1}-\\mathbf{y}) \\log{(1-\\psi)} - \\text{sum over columns}\\left( \\mathcal{X} \\odot \\log{\\boldsymbol{\\phi}} + (\\boldsymbol{1} - \\mathcal{X}) \\odot \\log{(\\boldsymbol{1}-\\boldsymbol{\\phi})} \\right).\n",
        "$$\n",
        "\n",
        "In this formula:\n",
        "\n",
        "1. We write $\\boldsymbol{1} - \\mathbf{y}$, where the bold $\\boldsymbol{1}$ stands for the $m\\times 1$ column vector with $1$'s in every entry. (_Hint_: You may simply write `1 - y` in your code.)\n",
        "2. We write $\\log{\\boldsymbol{\\phi}}$ for the entrywise logarithm of the $m\\times n$ matrix $\\boldsymbol{\\phi}$ obtained from your `phi_link` function, and similarly for $\\log{(\\boldsymbol{1}-\\boldsymbol{\\phi})}$. The bold $\\boldsymbol{1}$ stands for the $m\\times n$ matrix with $1$'s in every entry. (_Hint_: You may simply write `1 - phi` in your code.)\n",
        "3. We write $\\odot$ for the entrywise product of one $m\\times n$ matrix with another. (_Hint_: The entrywise product is given by the star operator `*`.)\n",
        "4. We write \"*sum over columns*\" for the function that does exactly that: It takes an $m\\times n$ matrix and sums over the columns to obtain an $m\\times 1$ column vector. (_Hint_: Look up the method `torch.sum` in [the docs](https://pytorch.org/docs/stable/generated/torch.sum.html). Summing over columns corresponds to the parameter `dim=1`.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz6fL1GhlT0f"
      },
      "source": [
        "### Problem 2 --- Implementing the vectorized model surprisal function\n",
        "\n",
        "In this problem, you will put everything together and implement your vectorized model suprisal function. Using the hints above (also see the code in the [the book](https://mml.johnmyersmath.com/stats-book/chapters/13-learning.html#mle-for-logistic-regression) where I implemented `I_model` for logistic regression) enter your implementation in the next cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-VyyPKS6a8G"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "def I_model(parameters, X, y):\n",
        "  None            # <-- replace `None` with your own code\n",
        "  None            # <-- replace `None` with your own code\n",
        "  return None     # <-- replace `None` with your own code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cqm0wE3moBE"
      },
      "source": [
        "Let's test whether your implementation is correct before we train the model on the email data. For this, you need initial values for the parameter vectors $\\boldsymbol{\\theta}_0$ and $\\boldsymbol{\\theta}_1$, as well as the parameter $\\psi \\in [0,1]$. In the next cell, I will do this for you, choosing random values for all these parameters, and then load them into a dictionary called `parameters`. Run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU6Rd4mOLgfP"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(57702)\n",
        "theta0 = torch.rand(size=(6,))\n",
        "theta1 = torch.rand(size=(6,))\n",
        "psi = torch.rand(size=(1,))\n",
        "parameters = {'theta0': theta0, 'theta1': theta1, 'psi': psi}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKf4kr53MB_A"
      },
      "source": [
        "Selecting good initial values for parameters can be tricky, and learning algorithms (like gradient descent) can be quite sensitive to initial parameter choice. Very often, initial parameters are _randomly_ chosen in some fashion. Here, we've chosen each of the $13 = 6 + 6 + 1$ initial values in the parameters from the uniform distribution on $[0,1)$.\n",
        "\n",
        "Let's have a look at our initial parameters, just out of curiosity. Run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjdD44OgMLdy",
        "outputId": "3e07c81e-207c-4e49-ba5f-023970f6c1b6"
      },
      "outputs": [],
      "source": [
        "print('Initial theta0 :', theta0)\n",
        "print('Initial theta1 :', theta1)\n",
        "print('Initial psi :', psi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgN3Yc78RXkv"
      },
      "source": [
        "Yup, those look like random numbers. Just like we were expecting! ðŸ¤˜\n",
        "\n",
        "Now, in the next cell, pass in the dictionary `parameters`, as well as the design matrix `X` and the label vector `y` into your `I_model` function. Save the output into the variable `surprisals`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDr7FeMgRdqH"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "surprisals = None       # <-- replace `None` with your own code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8ulKgEJTNkF"
      },
      "source": [
        "Now for the moment of truth. Run the next cell to check if your implementation of `I_model` is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWBqKUxXTSeM",
        "outputId": "0f1ab739-c265-4c8f-ab79-3255f0c556b8"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL TO CHECK YOUR ANSWERS\n",
        "\n",
        "prob_check(answers=[surprisals], prob_num=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcXp45qMXNKR"
      },
      "source": [
        "## Training the model and checking convergence\n",
        "\n",
        "Congratulations on making it this far! ðŸ˜€ ðŸ‘ As you can tell from the hard work that it took to write the  `I_model` function, it's one thing to write down a mathematical formula on paper, but an entirely different thing to code an efficient implementation.\n",
        "\n",
        "Now, assuming that `I_model` is correct, you will run stochastic gradient descent with `I_model` as the [loss function](https://mml.johnmyersmath.com/stats-book/chapters/11-optim.html#stochastic-gradient-descent) in order to minimize the cross entropy between the model distribution and the empirical distribution of the data. Thus, we are looking to **actually carry out** the learning process according to [Theorem 13.4](https://mml.johnmyersmath.com/stats-book/chapters/13-learning.html#equiv-obj-gen-thm) in the book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuJaY1n3ZBdk"
      },
      "source": [
        "### Problem 3 --- Training the model\n",
        "\n",
        "In the next cell, I am asking you to write **all** of the code to train your model using stochastic gradient descent.\n",
        "\n",
        "_Directions/hints/tips_:\n",
        "\n",
        "1. This will require you to choose appropriate values (on your own!) for the learning rate, the number of epochs, and the mini-batch size. To begin, just pick what seem like sensible values, knowing that you might have to return and choose different values to get the algorithm to converge. Collectively, these are called _hyperparameters_, and the process of choosing values for them is called called _hyperparameter tuning_.\n",
        "2. Use the code in [the book](https://mml.johnmyersmath.com/stats-book/chapters/13-learning.html#mle-for-logistic-regression) for inspiration. In particular, the part where I train a logistic regression model might be informative.\n",
        "3. The docs for our custom stochastic gradient descent routine are [here](https://github.com/jmyers7/math_stats_ml?tab=readme-ov-file#sgd-function-stochastic-gradient-descent), if you need them.\n",
        "4. Save the output of your SGD run into the variable `gd_output`.\n",
        "\n",
        "Even if your code is correct, there is the possibility that your algorithm blows up because you chose poor values for your hyperparameters. Be careful!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn11Sx5bt3Nq"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dZ1FDB3bBHi"
      },
      "source": [
        "### Problem 4 --- Checking convergence\n",
        "\n",
        "How do you know if stochastic gradient descent converged on good parameter values for $\\boldsymbol{\\theta}_0$, $\\boldsymbol{\\theta}_1$, and $\\psi$? Well, first, if the previous cell didn't blow up and throw an error, that's a good first sign. But in order to properly check convergence, we need to make a diagnostic plot.\n",
        "\n",
        "So, in the next cell, plot the cross entropy versus the number of gradient steps using our convenient `plot_gd` helper function. (See the book.) Again, you must write **all** the code. (See the book for help.) Choose descriptive names for the axis labels and the plot title, and be sure to include a legend with good descriptions. (**SEE THE DANG BOOK!!!**) By the way, the docs for the `plot_gd` function are [here](https://github.com/jmyers7/math_stats_ml?tab=readme-ov-file#plot_gd-function-plot-the-output-of-gradient-descent)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "ER1izYIXZ9rz",
        "outputId": "cbb9811b-3e3a-4bea-d033-809ff03052c1"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muuqiBYxb227"
      },
      "source": [
        "You may claim victory if the mean entropy over the last epoch is below a value of $3.5$. This value is represented in the plot as the _last_ orange dot. But to check its value _precisely_, run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydJdF0lGeCS6",
        "outputId": "0c4f452e-f4d5-49c7-cb22-7d06c65a22df"
      },
      "outputs": [],
      "source": [
        "gd_output.per_epoch_objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rwR_EHbeGJx"
      },
      "source": [
        "This code accesses the `per_epoch_objectives` attribute of the `gd_output` object produced by SGD, which is a $1$-dimensional PyTorch tensor containing the mean cross entropies over all the epochs. (Note that `gd_output` is an object of the custom class `GD_output` that I wrote specifically for our course. These objects track many other useful attributes. To see them all, see the docs [here](https://github.com/jmyers7/math_stats_ml?tab=readme-ov-file#gd_output-class-container-class-for-output-of-algorithms).)\n",
        "\n",
        "If the _last_ number is below $3.5$, you're golden. ðŸ˜Ž If not, then you need to go back to Problem 3 and select different values for your hyperparameters in order to get the value below $3.5$. (If you copied your hyperparameters from the book, then you might see a `nan`, which means \"not a number.\" Your algorithm blew up! Ha ha! Got ya! ðŸ˜ˆ ðŸ˜›)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0rVhLVaTQ_-"
      },
      "source": [
        "## Building the spam classifier\n",
        "\n",
        "We now want to turn our trained Naive Bayes model into a spam classifier. This means that, given a feature vector\n",
        "\n",
        "$$\n",
        "\\mathbf{x} = (x_1,x_2,\\ldots,x_6) \\in \\{0,1\\}^6\n",
        "$$\n",
        "\n",
        "containing observations of the six indicator random variables $X_1,X_2,\\ldots,X_6$ for the six words mentioned above, we want to generate a predicted value $\\hat{y}$ of the spam indicator random variable $Y$. If $\\hat{y}=1$, then the model predicts the email is spam, while if $\\hat{y}=0$, it is predicting non-spam.\n",
        "\n",
        "The spam classifier will take the form of the _predictor function_ given by\n",
        "\n",
        "$$\n",
        "h:\\{0,1\\}^6 \\to \\{0,1\\}, \\quad \\hat{y} = h(\\mathbf{x}) = \\operatorname*{argmax}_{y\\in \\{0,1\\}} p(y \\mid \\mathbf{x}).\n",
        "$$\n",
        "\n",
        "In other words, $h(\\mathbf{x})$ is equal to $\\hat{y}=1$ if\n",
        "\n",
        "$$\n",
        "p(y=1 \\mid \\mathbf{x}) > p(y=0 \\mid \\mathbf{x}),\n",
        "$$\n",
        "\n",
        "and it is equal to $\\hat{y}=0$ otherwise. So, if the (conditional) probability that an email is spam is greater than the probability that it is not spam, the predictor predicts spam. Pretty intuitive, right?\n",
        "\n",
        "These conditional probabilities come from the model probability distribution. So, we really should be writing\n",
        "\n",
        "$$\n",
        "p(y \\mid \\mathbf{x} ; \\ \\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1,\\psi)\n",
        "$$\n",
        "\n",
        "since they depend on the model parameters. But we won't.\n",
        "\n",
        "By definition we have\n",
        "\n",
        "$$\n",
        "p(y\\mid \\mathbf{x}) = \\frac{p(\\mathbf{x},y)}{p(\\mathbf{x})}.\n",
        "$$\n",
        "\n",
        "Since the feature vector $\\mathbf{x}$ is assumed given, the probability $p(\\mathbf{x})$ is constant with respect to $y$, and so we have\n",
        "\n",
        "$$\n",
        "\\operatorname*{argmax}_{y\\in \\{0,1\\}} p(y \\mid \\mathbf{x}) = \\operatorname*{argmax}_{y\\in \\{0,1\\}} p(\\mathbf{x},y).\n",
        "$$\n",
        "\n",
        "But recall that\n",
        "\n",
        "$$\n",
        "\\mathcal{I}_\\text{model}(\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1, \\psi; \\ \\mathbf{x},y) = -\\log\\left[p(\\mathbf{x},y)\\right].\n",
        "$$\n",
        "\n",
        "Since the negative logarithm function is strictly decreasing, we conclude that\n",
        "\n",
        "$$\n",
        "\\operatorname*{argmax}_{y\\in \\{0,1\\}} p(y \\mid \\mathbf{x}) = \\operatorname*{argmin}_{y\\in \\{0,1\\}} \\mathcal{I}_\\text{model}(\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1, \\psi; \\ \\mathbf{x},y),\n",
        "$$\n",
        "\n",
        "and so our predictor function is (equivalently) given by the formula\n",
        "\n",
        "$$\n",
        "\\hat{y} = h(\\mathbf{x}) = \\operatorname*{argmin}_{y\\in \\{0,1\\}} \\mathcal{I}_\\text{model}(\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1, \\psi; \\ \\mathbf{x},y).\n",
        "$$\n",
        "\n",
        "This is convenient because _we already have an implementation of the model surprisal function!_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYl2iOxCe05K"
      },
      "source": [
        "### Problem 5 --- Implementing the predictor function\n",
        "\n",
        "But, not only do we already have an implementation of the model surprisal function, we have a _vectorized_ implementation. So, this means that we may pass in an _entire_ design matrix into our predictor function $h$ to generate predictions over an entire dataset in one line of code.\n",
        "\n",
        "In the next code cell, please implement a vectorized predictor function using `I_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pqkbsjwnyMc"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "def h(X, parameters):\n",
        "  m = len(X)                          # size of the dataset\n",
        "  y_zeros = torch.zeros(size=(m,))    # a y-vector of all 0's\n",
        "  y_ones = torch.ones(size=(m,))      # a y-vector of all 1's\n",
        "  nonspam_surprisals = None           # <-- replace `None` with your own code\n",
        "  spam_surprisals = None              # <-- replace `None` with your own code\n",
        "  surprisals = torch.column_stack((nonspam_surprisals, spam_surprisals))\n",
        "  return torch.argmin(surprisals, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNEI4OPNoAkA"
      },
      "source": [
        "Before we test out our predictor function, we need to get the learned parameters from SGD. These are contained in the `parameters` attribute of the `gd_output` object produced by SGD. I will grab these parameters for you using \"dictionary comprehension,\" and load them into a dictionary called `learned_parameters`. Run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ8n26pKghFt"
      },
      "outputs": [],
      "source": [
        "learned_parameters = {name: param[-1] for name, param in gd_output.parameters.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn92uXxToSrJ"
      },
      "source": [
        "Let's have a look at the learned parameters, just to satisfy our own curiosity. Run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeLVE8WwoVv_",
        "outputId": "c0a65987-3bfc-4dc9-f405-cd3f4cd7ac2b"
      },
      "outputs": [],
      "source": [
        "learned_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISEOodTXoaJ0"
      },
      "source": [
        "So, those are the learned parameters discovered by SGD---they are the parameters that minimize the cross entropy from the model distribution to the empirical distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXQvA-8NqLs4"
      },
      "source": [
        "### Problem 6 --- Checking classification metrics\n",
        "\n",
        "Having written an implementation of the predictor function, let's check how well our model performs on the original dataset used for training (i.e., the _training set_). Note that, in the real world, we would not only check the performance on the training set, but also on _validation sets_ as described in the previous [programming assignment](https://github.com/jmyers7/stats-book-materials/blob/main/programming-assignments/assignment_12.ipynb).\n",
        "\n",
        "In the next code cell, use your predictor function `h` to generate predictions on the data in the original design matrix `X`. What parameters are you passing into `h`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5CBqE8onJR-"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "y_hat = None    # <-- replace `None` with your own code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frSEzyoBrHHQ"
      },
      "source": [
        "Having generated the predictions and loaded them into the vector `y_hat`, we now want to compute the various classification metrics described in Problem 6 of the worksheet in the [previous chapter](https://github.com/jmyers7/stats-book-materials/blob/main/worksheets/12-models.pdf). Conveniently, these metrics are already implemented in the [TorchEval](https://pytorch.org/torcheval/stable/) library!\n",
        "\n",
        "Run the next cell to import these metrics---take care to note which ones we are importing! For technical reasons, we also need to alter the data types of the `y` and `y_hat` vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_BKpIwwhDTj",
        "outputId": "45239b05-da78-4a66-8dc4-7f94e5951ece"
      },
      "outputs": [],
      "source": [
        "!pip install torcheval  # install the `torcheval` library\n",
        "from torcheval.metrics.functional import binary_accuracy, binary_precision, binary_recall, binary_confusion_matrix\n",
        "\n",
        "# cast the true label vector and predicted label vector to ints\n",
        "y_hat = y_hat.to(torch.int64)\n",
        "y = y.to(torch.int64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk1Sh-4FkTY9"
      },
      "source": [
        "In the next code cell, generate the confusion matrix for our spam filter. The format of the matrix output by TorchEval is:\n",
        "\n",
        "$$\n",
        "\\begin{array}{c|c|c}\n",
        "& \\hat{y}=1 & \\hat{y}=0 \\\\ \\hline\n",
        "y=1 & \\text{TP} & \\text{FN} \\\\\n",
        "y=0 & \\text{FP} & \\text{TN}\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "See [the docs](https://pytorch.org/torcheval/stable/generated/torcheval.metrics.functional.binary_confusion_matrix.html#torcheval.metrics.functional.binary_confusion_matrix) for the call signature of the `binary_confusion_matrix` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCqRJWgUhpml",
        "outputId": "341f729a-13a3-4981-9836-4c4db9f0459d"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq9iJU-vsWER"
      },
      "source": [
        "Finally, compute the _accuracy_, _precision_, and _recall_ scores using the functions imported from TorchEval. (Search the docs for the call signatures of these functions. Google them!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhjlW2iwh9g7",
        "outputId": "639885e0-a229-46d8-fea2-1049e801ffd5"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "accuracy = None         # <-- replace `None` with your own code\n",
        "precision = None        # <-- replace `None` with your own code\n",
        "recall = None           # <-- replace `None` with your own code\n",
        "\n",
        "print(f'accuracy:  {accuracy.item():0.4f}')\n",
        "print(f'precision: {precision.item():0.4f}')\n",
        "print(f'recall:    {recall.item():0.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsXVBgcCsveO"
      },
      "source": [
        "Assuming that you successfully trained your model in Problem 3 so that the cross entropy is $\\leq 3.5$, all three of the metrics in the previous cell should be $\\approx 95\\%$ or better. If that's what you see, then congrats! You're done! If not, then you need to return to Problem 3 and further tune the SGD hyperparameters in order to get scores on these metrics around $95\\%$."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
